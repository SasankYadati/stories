{
  
    
        "post0": {
            "title": "Computing Machinery and Intelligence",
            "content": "The Imitation Game . Can machines think? . This question begs one to define the words “machine” and “think”. Instead of defining them — which is seemingly easy, let’s replace the question with one that is very similar. Before that, we introduce the imitation game. . The game is played by three. A man, a woman and an interrogator. The interrogator is isolated from the other two and can ask each one of them questions, with a goal of identifying who the man and who the woman is. The man and woman, respond in a way so as to mislead the interrogator. We can assume the interrogator is oblivious to factors unconcerned with intellect such as voice, appearance and so forth. . . Suppose, we replace one of the man and woman with a machine. Can the interrogator now identify them wrongly as often as they do otherwise? . Critique of the New Problem . Is this new question a worthy one to investigate? . The imitation game has the setting that prevents the investigator from seeing, touching or hearing the players. This setting provides the advantage of drawing a distinction between physical and intellectual capabilities of the players. The idea is to not penalize the machine for not being human-like except for when we compare intellect. . Some might argue that odds are too heavily against the machine. Because, for a person it might be easier to pretend as a man/woman more than as a machine. Slowness and inaccuracy can give it away. Nevertheless, if a machine can play the game well enough, it can convince that it’s not a machine as well as the human player can convince otherwise. . The Machines Concerned in the Game . Even with the imitation game replacing our original question, we still have to define what a machine is. We wish to allow every kind of engineering technique and the possibility of the engineers not being able to describe the machine’s operations. To pin it down, we are interested in the kind of machines, which got us asking whether they can think, the digital computers. . One might ask why not perform the test with the existing digital computers. The short answer is that we are not looking whether all digital computers would fare well at the game. It’s not even whether the present digital computers would play well. We are only interested to know whether there are imaginary digital computers that would do well. . Digital Computers . A digital computer consists mainly of 3 parts. . Store | Executive Unit | Control The store is a collection of information. The executive unit carries out various individual operations, which vary from machine to machine. The table of operations that the machine can perform are stored as part of the store. The control makes sure the instructions are obeyed in the right order. The information in store is usually stored in small packets of fixed size. Numbers are systematically assigned to the parts of store to identify packets. | . There are digital computers being constructed at present just according to the principles we have described. If one desires to make a machine mimic the behavior of a human computer, all they have to do is describe formally what the human computer does in the form of an instruction table. We call the construction of instruction tables, programming. . An interesting variant can be the idea of a random element in a digital computer. It is not possible to determine whether a program involves a random element just by observing its behavior, simply because such effects can be produced — for example by making choices that depend on digits of pi. . When Charles Babbage planned the Analytical Engine, albeit incomplete, he shared the ideas of a digital computer. The fact that the Analytical Engine is equivalent to a digital computer but was completely mechanical tells us that the use of electricity cannot be of theoretical importance. The use of electricity in digital computers and brains is an implementation detail. Electricity is deemed useful in fast signalling and should be no surprise that it is used in computers and brains. . Universality of Digital Computers . Digital computers are essentially discrete state machines. The states in which it can be are definite and discrete enough for all practical purposes. Consider a discrete state machine, with a given initial state and input signals. It is possible to compute all the possible future states of this machine. This characteristic is reminiscent of Laplace’s Demon except that in case of a discrete state machine it’s much more practical. . . Although discrete, digital computers can occupy astronomical number of states. The machine in Manchester has about 2¹⁶⁵⁰⁰⁰ states. It’s storage capacity is about 165000. I believe Turing was referring to this. Storage capacity can be thought as number of bits, as we know a bit encodes 2 states. . Given the initial state and set of input signals, a digital computer is capable of computing all the possible future states. If the digital computer is sufficiently fast and has adequate storage capacity, it can mimic any discrete state machine. The two human players in the imitation game can be replaced by a discrete state machine and a digital computer, which mimics it, and the interrogator would not be able to tell them apart. . This special property of digital computers, that they can mimic any discrete-state machine, is described by saying that they are universal machines. . The question can be restated as follows. Can we take a particular digital computer, give it adequate storage capacity, necessary improvements in speed and an appropriate program, and expect it to play satisfactorily in the imitation game against a human? . Contrary Views on the Main Question . I believe that in about fifty years’ time it will be possible, to programme computers, with a storage capacity of about 10⁹, to make them play the imitation game so well that an average interrogator will not have more than 70 per cent chance of making the right identification after five minutes of questioning. . … Nevertheless I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted. . Mathematical Objection . Gödel’s theorem is one of the results that describes the limitations of discrete state machines. It shows that in any sufficiently powerful logical system that is consistent, there will be statements that are true but cannot be proved within the system. Gödel, Church and Turing independently came up with different proposals for an exact mathematical definition of computable functions, and consequently, of decidable sets. These proposals, though, all turned out to be equivalent. So for the sake of convenience we will refer to the latter work since it’s directly related to machines. The results of the work essentially imply that machines, even with infinite storage capacity, cannot give answers to certain questions. It will either give a wrong answer, or cannot arrive at an answer no matter how much time is allowed. The nature of these questions is that the answers Yes/No are appropriate. The questions the machines will fail on are of this type, “Consider the machine specified as follows . . .Will this machine ever answer “Yes” to any question?” The dots are to be replaced by a formal description of some machine. Suppose the machine described is similar to the machine under interrogation, it can be shown that the answer is either wrong or unforthcoming. For more information, check undecidability. The objection is that no such limitation exists for humans, albeit this statement has no proof. . There would be no question of humans triumphing simultaneously over all machines. In short, then, there might be people cleverer than any given machine, but then again there might be other machines cleverer again, and so on. . Other objections: . Turing also presents various arguments of the following nature: . Thinking is a function of an immortal soul. God has given an immortal soul to every human, but not to any other animal or machines. | Consciousness enables thinking and we cannot test for consciousness through the imitation game. | Various disabilities of machines: machines can do all of that, but not X. | Lady Lovelace’s objection that a machine can never do anything really new. | The continuity of the nervous system prevents the machines, which are discrete in nature, to mimic its behaviour. | Behaviour is informal and it is not possible to produce a set of rules purporting to describe what a man should do in every conceivable set of circumstances. | Arguments from people, who believe in extrasensory perception. | . Learning Machines . I have no very convincing arguments of a positive nature to support my views. If I had I should not have taken such pains to point out the fallacies in contrary views. Such evidence as I have I shall now give. . Human minds, most times, seem to be “sub-critical,” i.e., when an idea is presented to such a mind, it will on average give rise to less than one idea in reply. Other times, they seem to be super-critical, when presented with ideas give rise to a whole “theory” consisting of secondary, tertiary and more remote ideas. But this support isn’t good enough for the claims made early in the previous section. . Can a machine be super-critical? . The problem is mainly programming. Engineering advances are necessary but insufficient. Storage capacity of about 10¹⁵ (estimated human brain’s storage capacity) should be more than sufficient and it should not require us to increase the operational speed. Our problem then is mainly to programme these machines to play the game. We mainly see three components when we observe the growth of a child’s mind into an adult’s mind. . Initial state of mind, say, at birth | Education it has been subjected to | Other experiences it has been subjected to Producing a program to simulate a child’s mind and subjecting it to necessary experience is just as appropriate as producing one that simulates an adult mind. The program and the experience are closely connected. This idea is in fact analogous to evolution, where the judge evaluating a machine’s progress corresponds to natural selection. However, this process is likely faster and more efficient than natural selection. The use of punishments and rewards, which are traditionally associated with teaching, can at best be a part of training process if there are “unemotional” channels of communicating the punishments and rewards. How complex should the child machine be? Should it just be rudimentary and consistent with the general principles? Or should it have a complete system of logical inference? In the latter, system could be filled with well-established facts, conjectures, theorems, authority issued statements etcetera. The machine may produce its own propositions on top of it all may be by using scientific induction. If the idea of a learning machine sounds paradoxical, it could help to understand that lifetime of some of the machine’s rules are short-lasting. They deem more useful because of their changeable nature than being enforcers of said constraints. The teacher of the learning machine will be mostly ignorant of the machine’s internals. We should be able to teach the machine to do something without being too detailed on how to do it. What may seem to us as random behaviour — owing to the deviation from deterministic behaviour, is presumably intelligent behaviour. The inclusion of randomness in a learning machine is useful especially when solving a problem with multiple solutions. It is unnecessary to keep track of the tried solutions. Since there are probably a lot of solutions for the learning problem, random process is better than systematic. Hopefully, machines will soon compete with men intellectually. It could be by starting out on abstract tasks like playing chess (popular opinion) or it could be by providing the machine sensory equipment followed by a process similar to that of teaching a child. The right answer is unknown and both should be tried. We can only see a short distance ahead, but we can see plenty there that needs to be done. . |",
            "url": "https://sasankyadati.github.io/stories/computation/intelligence/2020/03/03/Computing-Machinery-and-Intelligence.html",
            "relUrl": "/computation/intelligence/2020/03/03/Computing-Machinery-and-Intelligence.html",
            "date": " • Mar 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Attacking Machine Learning Models",
            "content": "Machine Learning is an incredibly exciting technology. Many systems powered by Machine Learning are helping in making predictions and decisions for most businesses and organizations. The impact of such models is undeniable and significant. These models face many security issues nevertheless. It turns out such models could be fooled using some smartly crafted inputs. What’s worse is that crafting such inputs is not so hard at all. You can go ahead, hide your model and data. It’s still possible! . The Implications! . Oh, the implications are of significance definitely. Imagine a self driving car being fooled by a modified sign board. All one needs to do is print the adversarial image of the sign on a paper and stick it over. This is just one example of exploiting such weakness and I will leave it to your imagination where situations far worse are possible! . . Adversarial Attacks . Before we get any technical, here’s an informal introduction to adversarial attacks. This write-up is in no way a rigorous review of this field and is written only to expose you to some aspects of adversarial attacks in machine learning. Hope this inspires you even just a little bit to explore more about this field. Wew! With that being said, let’s dive in. . Adversarial attacks are basically when an attacker or an adversary uses adversarial examples to trick models into misclassifying them when such a misclassification is not expected of them. . What are the reasons for such a vulnerability? How do we come up with adversarial examples? How can we defend our models against such attacks? . Before we seek answers to these questions, lets get over with some formalities! . Adversarial attacks can be classified based on what the attacker knows about the model and if the attacker wants to target the model to predict a particular class or not. More formally, there are black-box attacks where the attacker knows nothing about the architecture or the parameters of the model but has access to using it to make predictions. White-box attacks are just the opposite — attacker knows the model’s internals. If the attacker wants to target a particular class to be the output for the adversarial input, we call that a targeted attack and non-targeted otherwise. . So, why does such a vulnerability exists? . First things first: this misclassification is not due to overfitting. Models which have high performance results on test sets are vulnerable as well! Adversarial examples fooling models basically implies that models are not robust to calculated noise. Small non-perceivable perturbations in the inputs are a challenge to the model. The reason for this is somewhat intuitive. I will try my best to explain this. Let us consider the problem of image classification. Consider a hypercube of nnn dimensions. All possible input images of size n exist in this hypercube. A classifier will “learn” to divide this hypercube into kkk partitions where kkk is the number of classes. Naturally, each class is a distribution on this hypercube and all the classes are mutually exclusive. In nature and reality, most of the points in the hypercube do not occur and hence are fake images. But, the classifier has learnt a mapping from all the points in the hypercube to a probability distribution over the classes. An adversarial image of image xxx is image yyy, subject to ∣x−y∣≤ϵ lvert x - y rvert ≤ epsilon∣x−y∣≤ϵ (ϵ epsilonϵ is a small amount of noise), where yyy is classified differently from xxx. Under these assumptions, we can see that adversarial examples always exist. The classifier will try to classify the input even if the input doesn’t occur in the natural distribution. But, theoretically deep neural networks are able to represent functions that are resistant to adversarial examples. Remember universal approximation theorem?! So, the learning algorithms are the real culprit here. They simply are not told to be wary of adversarial examples. . . How to generate Adversarial Examples? . We are already past the hardest part. We will discuss few methods to generate adversarial examples below and the key idea here is to use the gradient information. . Fast Gradient Sign Method . For a given example xxx, generate adversarial example x′x&amp;#x27;x′ as follows: . x′=x+ϵ∗sign(∂J∂x)x&amp;#x27; = x + epsilon * sign( frac{ partial J}{ partial x})x′=x+ϵ∗sign(∂x∂J​) . sign(c)sign(c)sign(c) returns −1-1−1 if ccc is negative and +1+1+1 otherwise of the input. ϵ epsilonϵ is the amount of perturbation. JJJ is the cost function. This can be intuitive once you notice that we are moving the image in the direction of increasing gradient in steps of size ϵ epsilonϵ. This is a one shot method to generate an adversarial example. ϵ epsilonϵ is a trade off between similarity to xxx and success of an adversarial attack using x′x&amp;#x27;x′. . Basic Iterative Method . We extend the idea of the above method and iteratively perform the update for x′x&amp;#x27;x′ and clip the output pixel wise so as to keep final x′x&amp;#x27;x′ within ϵ epsilonϵ neighborhood. . x′=clip(x+ϵ∗sign(∂J∂x))x&amp;#x27; = clip(x + epsilon * sign( frac{ partial J}{ partial x}))x′=clip(x+ϵ∗sign(∂x∂J​)) . Number of iterations is a hyper-parameter, that trades off between computational speed and a successful attack. So far, we have discussed non-targeted and white box attacks. We will next look at a targeted attack. . Targeted Fast Gradient Sign Method . We choose a target label and get a clean image x from which we would like to generate an adversarial example, x’. Ideally, this image belongs to a class that is perceptually close to our target but belongs to a different class. In the update step, the cost JJJ is calculated against the target class rather than the class of xxx. . x′=clip(x−ϵ∗sign(∂J∂x))x&amp;#x27; = clip(x - epsilon * sign( frac{ partial J}{ partial x}))x′=clip(x−ϵ∗sign(∂x∂J​)) . We apply the standard gradient descend as indicated by the negative sign so as to get closer to the target. This method can be extended as an iterative process as well. We have discussed methods to generate targeted and non-targeted white-box attacks. What if the model is a black box and we do not know the gradient information? Turns out, the adversarial examples are usually not model/architecture specific. So, you can go ahead and create your own model, generate adversarial examples using your model and they will succeed in attacking a different model trained on the same task. . What can we do to defend our models? . Hide your gradients! . If you observe all the equations above, they need the gradient to compute the adversarial examples. So, if we try to hide these gradients, the model should be safe against attacks. This is not true though. Masking the model’s gradients doesn’t really make the model more robust. Instead, it just makes the attack a little harder to perform, which could be either through a substitution model or randomly guessing adversarial points. Experiments showed that different models trained on similar tasks are vulnerable to similar adversarial examples. . Be prepared with Adversarial Training . Aha! Since we can generate adversarial examples for a given task, what if we use them to train the model along with the regular dataset? It is shown to improve classification accuracies on adversarial examples. However, they are still vulnerable to black-box attacks. They improve defense but the idea to include most adversarial examples in training is unrealistic. Our journey in finding a practical and reasonable defense continues… . Acknowledging Ignorance . What if we assign a “don’t know/null” class for the model and let the model predict this class for the images that it is not familiar with? Our hope is that the model would predict this class for adversarial examples. This method aims to block the transferability property of adversarial examples. This is easier said than done. I recommend training a model with such a class! How would you gather data for such a class? How many can you or should you gather? How many are enough? And, how practical is it to do something like that? To model such a formulation is not hard but the practicality of training such models is something I haven’t tried myself. However, this method is considered an effective defense method against adversarial attacks. . Conclusion . This article is no way a rigorous write up on this field. I took the liberty to choose only a small set of methods and ideas to discuss here. I’m in no way saying these are superior to the ones that aren’t discussed here. In fact, I recommend you explore other proposed methods as well or maybe come up with one of your own! Machine Learning researchers got really creative and proposed a lot more methods to generate adversarial examples and to defend against such examples. They all have their arguments for the reasons behind such vulnerabilities but are unanimous in acknowledging the importance of this field. And finally, I hope you, dear reader, are little more curious than you were before about the field of adversarial attacks. Whether you develop machine learning systems or research on machine learning algorithms, I hope you are convinced that adversarial attacks are worthy to be considered in your work. Adios! .",
            "url": "https://sasankyadati.github.io/stories/deep%20learning/2019/06/15/Attacking-Machine-Learning-Models.html",
            "relUrl": "/deep%20learning/2019/06/15/Attacking-Machine-Learning-Models.html",
            "date": " • Jun 15, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Deep Learning can only do so much",
            "content": "Prologue . Deep Learning has revolutionized machine capabilities by leveraging the advancements in computing power and the rise in amounts of data available. From object recognition to recommendation engines and from translation systems to fraud detection, we have witnessed state-of-the-art performances using Deep Learning. . Consequently, Deep Learning powered applications have become a reality in no time. Apps on your smartphone are probably running Deep Learning algorithms locally while getting better with time and usage. Obviously, there is so much hype around Deep Leaning thanks to the various tech companies and media. In fact, most of the hype is justified given its impact. However, I believe it is vital to understand what Deep Learning can and cannot do. . So, what is Deep Learning anyway? . Deep Learning is essentially a statistical technique used to identify patterns given data. This is achieved using neural networks which are nothing but stacks of layers of neurons. Each neuron is a function whose parameters are to be determined by the optimization process. So essentially, each layer computes a transformation on the input and passes it forward onto the next layer. . . To sum it up, you have a large bunch of examples including the input and the output. You feed the input, compute the output by performing transformations at each layer, compare this output with the actual output and depending on the difference we adjust the parameters of neurons in different layers. This process repeats until we achieve the desired performance. . Although I have obscured many details, it still is a simple idea. Yet, unbelievably effective. Theoretically, Deep Learning is powerful enough to represent any deterministic mapping between a set of inputs and outputs. But, you probably shouldn’t read too much into it, as many other factors come into play in practice. . Limitations of Deep Learning . Data hungry . Deep Learning models work well only when they have large amounts of data available to them. . Give me a X, Give me a Y, Give me a lot of them! . In fact, the more complex transformations you want it to learn (assuming it can), the bigger the network has to be and more the data it requires so as to not fit on some irrelevant relationships and patterns. This is mostly in contrast with how humans learn. A child doesn’t need millions of examples to classify a hot dog from not a hot dog. . Adversarial examples hurt the performance . Using the idea of gradient ascend, it is very easy to trick the model to predict really high probabilities for incorrect classes. This is as simple as taking an image of class A and adding to it the gradient of an image of class B. Although it almost has no visual difference to us, deep learning models fail to classify them. Many strategies have been proposed to prevent adversarial attacks, but almost all of them defend only some subset of these attacks. . Generalization capabilities are limited . Deep Learning performs well on test data only if the test data distribution resembles that of the training data. Any deviation in the test input thrown at it will result in poor performance. To quote Francois Chollet, . …our models can only perform local generalization, adapting to new situations that must stay very close from past data, while human cognition is capable of extreme generalization, quickly adapting to radically novel situations, or planning very for long-term future situations. . I could not put that in better words. Deep Learning can only generalize to something new given it is similar to the data it has seen. Humans, on the other hand, are adaptive to novel situations. . Restrictions on transformations it can use . We have learned that each layer in a neural network applies a geometric transformation to the input and passes it forward to the next layer. Deep Learning uses gradient-based optimizations to learn the right values for the parameters of the transformations. This imposes a restriction on what kind of transformations we could use. Since, the transformations need to be differentiable, they need to be smooth and continuous as well. This is quite a strong restriction. . Trouble with Hierarchical Structure . There is no way for a Deep Learning model to deal with hierarchical structures. It only learns correlations among features that are flat and considered equal. This is a serious problem, considering what Noam Chomsky believes: natural languages have hierarchical structures, where large structures are recursively made out of smaller ones. . . This is why RNNs have trouble extending what they have learned to unfamiliar examples. They assume natural languages to be nothing but a set of sequences and thereby perform poorly when they come across complex sentences. . . In this post, I have covered some of the limitations of Deep Learning. I believe it is important for somebody who wants to work with it to know what it can and cannot do. There is a lot of research going on to overcome these shortcomings. Most of these approaches are in contrast with one another which makes it all the more exciting. With that being said, I can’t wait to see what the future in Artificial Intelligence research holds for us. Cheers! .",
            "url": "https://sasankyadati.github.io/stories/deep%20learning/2018/04/20/Deep-Learning-Can-Only-Do-So-Much.html",
            "relUrl": "/deep%20learning/2018/04/20/Deep-Learning-Can-Only-Do-So-Much.html",
            "date": " • Apr 20, 2018"
        }
        
    
  
    
        ,"post3": {
            "title": "TensorFlow",
            "content": "Introduction . TensorFlow is an open-source library for machine intelligence developed by Google. It is a computational library with a wide range of functionality. However, its main purpose is to implement Machine Learning algorithms. TensorFlow is extremely popular relative to other Deep Learning libraries due to many reasons. One of the most important reasons being its ability to facilitate both research and production. This was not possible with the other libraries. Researchers and developers had to use one language for research and prototyping and some other language to deploy their model into production. Needless to say, TensorFlow took good care of this. . Research AND Production?! TensorFlow got it covered. . There are many other reasons for someone to choose TensorFlow over other libraries. Some of them include Python interface, TensorBoard (kick-ass visualizations!) and Auto Differentiation. . The basics . Any TensorFlow program essentially contains 2 components : Computational Graph and Session. Computational Graphs are a means to specify computations that need to be carried out in our model. They are directed graphs, where nodes represent operations and edges represent tensors and the direction in which they move (hence, the name TensorFlow!). Once we define our graph, we can run it by creating a session. We can execute the whole graph or just a part of it. Session allocates memory for the graph and the variables. . (Note : TensorFlow has a new feature called Eager Execution which is a more Pythonic way of doing things. It is imperative in nature contrast to the graph-and-session approach, which is declarative.) . . In the above graph, you can see nodes corresponding to multiplication and addition operations. You can also see how data flows in the graph from one node to another. After defining such a graph, we can run it (or parts of it) in a session. . Tensors, Constants, Variables and Placeholders . import tensorflow as tf . Tensors are best understood as a means of generalizing scalars, vectors and matrices. Scalars are 0-dimensional, vectors are 1-dimensional and matrices are 2-dimensional. . Tensors are n-dimensional arrays. . Extending this idea, tensors are nothing but n-dimensional. If n is 0 they are scalars, if n is 1 they are vectors and so on. The rank of a tensor is nothing but the number of dimensions of the tensor. . Creating constants in TensorFlow is simple. . # A is a 1D tensor constant A = tf.constant([2,6], name=&quot;A_vector&quot;) # B is a 2D tensor constant B = tf.constant([[10, 11], [42, 33]], name=&quot;B_matrix&quot;) . You can create constants using various other operations like tf.zeros(...) , tf.ones(...) and tf.random_normal(...). . Constants’ values cannot be changed and are stored in the graph’s definition. They are loaded every time you load the graph. Variables on the other hand, are mutable and are stored separately from the graph definition. . # Variables can be created using tf.get_variable(...) function tf.get_variable( name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, use_resource=None, custom_getter=None, constraint=None ) # Let us create a variable &quot;weights&quot; of shape 10x10 filled with zeros. weights = tf.get_variable(&quot;weights&quot;, shape=(10,10), initializer=tf.zeros_initializer()) . Constants are immutable and are stored in the graph definition. Variables are mutable and are stored separately from the graph. . Before using the variables you have to initialize them in a session. For this, we usually create an initializer operation and run this operation in a session. One operation is enough to initialize all variables, although you can initialize them individually. . # Define variables a = tf.get_variable(...) b = tf.get_variable(...) # Define an initializer operation init_op = tf.global_variables_initializer() # Create a session with tf.Session() as sess: # Run the init operation sess.run(init_op) # Use variables sess.run(a) sess.run(b) . When we are defining our graph, we will not always know what values we need for certain computations. To define a computation without specifying the values, we use placeholders. It is similar to defining F(x,y) without specifying the values of x and y . . Placeholders allow us to define computations without providing values. . Let us define f(x,y) = x*y where x and y are placeholders. . # Define two placeholders x = tf.placeholder(tf.float32, shape=[2,2]) y = tf.placeholder(tf.float32, shape=[2,2]) # Define f to be x*y f = x*y # Create a session with tf.Session() as sess: x_value = [[1,2],[3,4]] y_value = [[5,6],[7,8]] # To compute f, we need to supply the values for the placeholders x and y f_value = sess.run(f,{x:x_value , y:y_value}) . As you can see in the code above, we need to supply values for placeholders if we want to evaluate f(x,y). . TensorFlow has many operations ranging from matrix inversion to advanced optimizations. We will cover basic operations by implementing a linear regression model. . Linear Regression in TensorFlow . Linear Regression is basically finding the best fit line through a set of points. We will create two placeholders x and y whose values are later supplied in a session. We have weights w and bias b , whose optimal values need to be found by an optimizer. So, these will be variables. We will define our model f as f(x) = w * x + b. . Have you ever fit a line through data? It’s liberating. . x = tf.placeholder(tf.float32, shape=[500]) # 500 values for x y = tf.placeholder(tf.float32, shape=[500]) # 500 values for y w = tf.get_variable(&#39;w&#39;, shape=[1,1], initializer=tf.zeros_initializer()) # Variable w initialized with 0&#39;s b = tf.get_variable(&#39;b&#39;, shape=[1,1], initializer=tf.zeros_initializer()) # Variable b initialized with 0&#39;s init_op = tf.global_variables_initializer() # Variables initializer operation f = w*x + b # Our model . Now, all that is left is to define a loss function and an optimizer. We will then run the optimizer operation in a session a few times and voila! We will have the model trained. . # Define loss function as sum of squares loss = tf.reduce_sum(tf.square(y-f))/500 # Define Gradient Descent optimizer with minimization of loss as objective optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0003).minimize(loss) ## Generate random data for training x_values = [random.randint(1,50) for _ in range(500)] y_values = [(20+random.random())*x_val+(2+random.random()) for x_val in x_values] ## Create a session with tf.Session() as sess: # Run the init operation sess.run(init_op) # 2000 Epochs for i in range(2000): # Run loss and optimizer operations c,_ = sess.run([loss,optimizer], {x:x_values,y:y_values}) if(i%100==0): # Print loss every 100 epochs print(&quot;Epoch : {} , Loss : {}&quot;.format(i,c)) # Finally, print optimized values of w and b print(&quot;w=&quot;.format(sess.run(w))) print(&quot;b=&quot;.format(sess.run(b))) . Conclusion . I hope you have gained a good sense of how TensorFlow works! You pretty much know the basic constructs and can further build-up to develop awesome applications. I wish to write a post on TensorBoard soon, covering the visualization aspects of TensorFlow. Until then! .",
            "url": "https://sasankyadati.github.io/stories/tensorflow/2018/04/05/TensorFlow.html",
            "relUrl": "/tensorflow/2018/04/05/TensorFlow.html",
            "date": " • Apr 5, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am currently a Software Engineer at Microsoft. I am generally interested in solving intelligence and particularly in Deep Learning and Reinforcement Learning. Solving intelligence is going to be the biggest event in the history of humanity and it’s important we make sure advanced AI systems are beneficial. .",
          "url": "https://sasankyadati.github.io/stories/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sasankyadati.github.io/stories/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}